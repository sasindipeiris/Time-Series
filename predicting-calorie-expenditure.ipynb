{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91716,"databundleVersionId":11893428,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.preprocessing as sp\nfrom sklearn.model_selection import KFold, RepeatedKFold\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV, LinearRegression, BayesianRidge\n\n# Ignore all warnings\nimport warnings  \nwarnings.simplefilter(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:37:23.039816Z","iopub.execute_input":"2025-05-20T15:37:23.040058Z","iopub.status.idle":"2025-05-20T15:37:29.584783Z","shell.execute_reply.started":"2025-05-20T15:37:23.040034Z","shell.execute_reply":"2025-05-20T15:37:29.584188Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"**CatBoost Regressor** is a gradient boosting algorithm **built on decision trees**, specifically optimized for handling numerical and categorical data in regression tasks. Unlike many other boosting frameworks, CatBoost uses an innovative technique called *ordered boosting*, which prevents target leakage during training, and *efficient encoding* of categorical variables via target statistics without manual preprocessing. It supports GPU acceleration, symmetrical tree structures for faster inference, and strong regularization capabilities to minimize overfitting. These features make CatBoost Regressor highly effective in modeling complex, non-linear relationships with minimal tuning, while maintaining high accuracy and stability across diverse datasets.\n\n**Ridge Regression**, available in `sklearn.linear_model` as `Ridge`, is a linear regression model enhanced with L2 regularization to address issues like overfitting and multicollinearity. Unlike standard linear regression, Ridge minimizes a loss function that combines the sum of squared residuals with a penalty term proportional to the square of the model coefficients. This penalty, known as L2 regularization, constrains the magnitude of the coefficients by adding $\\alpha \\| \\beta \\|^2_2$ to the cost function, where $\\alpha$ is the regularization strength. The effect of L2 regularization is to shrink large weights, reducing model variance without eliminating any variables entirely. This leads to more stable and generalizable predictions, especially in datasets with highly correlated or numerous features. Additional tools like `RidgeCV` allow automatic selection of the best regularization parameter using cross-validation. Compared to `LinearRegression`, which uses no regularization, or `BayesianRidge`, which adds probabilistic interpretation, Ridge offers a controlled and computationally efficient approach for regularized linear modeling.\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# define RMSLE\ndef rmsle_score(y, preds):\n    y = np.maximum(0, y)\n    preds = np.maximum(0, preds)\n    return np.sqrt(np.mean((np.log1p(preds) - np.log1p(y)) ** 2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:37:35.758987Z","iopub.execute_input":"2025-05-20T15:37:35.759672Z","iopub.status.idle":"2025-05-20T15:37:35.763340Z","shell.execute_reply.started":"2025-05-20T15:37:35.759651Z","shell.execute_reply":"2025-05-20T15:37:35.762725Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"SEED = 42          # Set random seed for reproducibility of results\nN_SPLITS = 5       # Number of folds for cross-validation (5-fold CV)\nN_REPEATS = 3      # Number of times to repeat cross-validation with different splits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:37:38.468328Z","iopub.execute_input":"2025-05-20T15:37:38.468592Z","iopub.status.idle":"2025-05-20T15:37:38.472464Z","shell.execute_reply.started":"2025-05-20T15:37:38.468571Z","shell.execute_reply":"2025-05-20T15:37:38.471746Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_df=pd.read_csv(\"/kaggle/input/playground-series-s5e5/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/playground-series-s5e5/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:37:41.855434Z","iopub.execute_input":"2025-05-20T15:37:41.855704Z","iopub.status.idle":"2025-05-20T15:37:42.965574Z","shell.execute_reply.started":"2025-05-20T15:37:41.855683Z","shell.execute_reply":"2025-05-20T15:37:42.965033Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"print(\"----Train data----\")\nprint(train_df.isnull().sum())\nprint(\"=\"*20)\nprint(\"----Test data----\")\nprint(test_df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:37:44.914590Z","iopub.execute_input":"2025-05-20T15:37:44.915077Z","iopub.status.idle":"2025-05-20T15:37:44.987747Z","shell.execute_reply.started":"2025-05-20T15:37:44.915053Z","shell.execute_reply":"2025-05-20T15:37:44.987142Z"}},"outputs":[{"name":"stdout","text":"----Train data----\nid            0\nSex           0\nAge           0\nHeight        0\nWeight        0\nDuration      0\nHeart_Rate    0\nBody_Temp     0\nCalories      0\ndtype: int64\n====================\n----Test data----\nid            0\nSex           0\nAge           0\nHeight        0\nWeight        0\nDuration      0\nHeart_Rate    0\nBody_Temp     0\ndtype: int64\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Add new features\ndef add_new_features(df):\n    # # BMI\n    # df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2)\n    # # Duration / Heart Rate\n    # df[\"Duration_per_HeartRate\"] = df[\"Duration\"] / (df[\"Heart_Rate\"] + 1e-5)\n    # # Intensity\n    # df['Intensity'] = df[\"Heart_Rate\"] / (df[\"Duration\"] + 1e-5)\n    # # Other\n    # df[\"Duration_x_HeartRate\"] = df[\"Duration\"] * df[\"Heart_Rate\"]\n    df['Calories_Burned'] = np.where(\n        df['Sex'] == 'male',\n        (-55.0969 + (0.6309 * df['Heart_Rate']) + (0.1988 * df['Weight']) + (0.2017 * df['Age'])) / 4.184 * df['Duration'],\n        (-20.4022 + (0.4472 * df['Heart_Rate']) - (0.1263 * df['Weight']) + (0.074 * df['Age'])) / 4.184 * df['Duration']\n    )\n    return df\n\ntrain_df = add_new_features(train_df)\ntest_df = add_new_features(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:37:48.545960Z","iopub.execute_input":"2025-05-20T15:37:48.546562Z","iopub.status.idle":"2025-05-20T15:37:48.662537Z","shell.execute_reply.started":"2025-05-20T15:37:48.546538Z","shell.execute_reply":"2025-05-20T15:37:48.661759Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# create Duration class column\nbins = list(np.arange(1, 40, 5))\nlabels = [f'{b}-{b+4}' for b in bins[:-1]]\n\ntrain_df['Duration_class'] = pd.cut(train_df['Duration'], bins=bins, labels=labels, right=False)\ntest_df['Duration_class'] = pd.cut(test_df['Duration'], bins=bins, labels=labels, right=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:37:52.416779Z","iopub.execute_input":"2025-05-20T15:37:52.417071Z","iopub.status.idle":"2025-05-20T15:37:52.459274Z","shell.execute_reply.started":"2025-05-20T15:37:52.417052Z","shell.execute_reply":"2025-05-20T15:37:52.458741Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"This code creates a **new categorical column** called `Duration_class` in both the training and testing DataFrames by grouping the continuous `Duration` values into intervals or “bins.” The bins are defined from 1 to 40 in steps of 5 (e.g., 1-5, 6-10, etc.), and each `Duration` value is assigned a label corresponding to the interval it falls into. Using `pd.cut` with `right=False` means each bin includes the left edge but excludes the right edge. This transformation converts the numeric `Duration` into meaningful categories, which can help models capture patterns related to different duration ranges more easily.\n","metadata":{}},{"cell_type":"code","source":"# create age class column\nbins = list(np.arange(1, 90, 5))\nlabels = [f'{b}-{b+4}' for b in bins[:-1]]\n\ntrain_df['age_class'] = pd.cut(train_df['Age'], bins=bins, labels=labels, right=False)\ntest_df['age_class'] = pd.cut(test_df['Age'], bins=bins, labels=labels, right=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:37:55.613933Z","iopub.execute_input":"2025-05-20T15:37:55.614505Z","iopub.status.idle":"2025-05-20T15:37:55.655836Z","shell.execute_reply.started":"2025-05-20T15:37:55.614479Z","shell.execute_reply":"2025-05-20T15:37:55.655195Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# target encoding\n# groubby --> Sex, Age_class, Duration_class\ngroup_encod = train_df.groupby(['Sex', 'age_class', 'Duration_class'])['Calories'].median().reset_index()\ngroup_encod.rename(columns={'Calories': 'Calories_encoded'}, inplace=True)\n\ntrain_df = train_df.merge(group_encod, on=['Sex', 'age_class', 'Duration_class'], how='left')\ntest_df = test_df.merge(group_encod, on=['Sex', 'age_class', 'Duration_class'], how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:37:58.516059Z","iopub.execute_input":"2025-05-20T15:37:58.516344Z","iopub.status.idle":"2025-05-20T15:37:58.837848Z","shell.execute_reply.started":"2025-05-20T15:37:58.516324Z","shell.execute_reply":"2025-05-20T15:37:58.837051Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"This code performs **target encoding** by grouping the training data based on the categories `Sex`, `age_class`, and `Duration_class`. For each group, it calculates the **median** of the target variable `Calories` and saves it as a new value called `Calories_encoded`. Then, it merges this encoded information back into both the training and testing DataFrames, adding a new column `Calories_encoded` that contains the median calorie value for each group. This way, the model gets additional useful information about how calories relate to combinations of sex, age group, and duration category.\n","metadata":{}},{"cell_type":"code","source":"train_df['Sex'] = train_df['Sex'].map({'male': 1, 'female': 0}).astype(\"float32\")\ntest_df['Sex'] = test_df['Sex'].map({'male': 1, 'female': 0}).astype('float32')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:38:02.029035Z","iopub.execute_input":"2025-05-20T15:38:02.029641Z","iopub.status.idle":"2025-05-20T15:38:02.092841Z","shell.execute_reply.started":"2025-05-20T15:38:02.029610Z","shell.execute_reply":"2025-05-20T15:38:02.092117Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder  # Import OneHotEncoder for converting categorical variables to binary columns\n\ncat_cols = ['Duration_class', 'age_class']  # Specify categorical columns to encode\n\nencoder = OneHotEncoder(sparse=False, drop=None, handle_unknown='ignore')  \n# Create encoder instance:\n# sparse=False returns a dense array instead of sparse matrix\n# drop=None means no category is dropped (all encoded)\n# handle_unknown='ignore' avoids errors if new categories appear in test data\n\n# train data\nencoded_train = encoder.fit_transform(train_df[cat_cols])  \n# Fit encoder on training data and transform categorical columns into one-hot encoded arrays\n\nencoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(cat_cols))  \n# Convert encoded numpy array into a DataFrame with proper column names\n\ntrain_df = pd.concat([train_df.drop(columns=cat_cols), encoded_train_df], axis=1)  \n# Drop original categorical columns from train_df and add the new one-hot encoded columns\n\n# test data\nencoded_test = encoder.transform(test_df[cat_cols])  \n# Transform test data using the already fitted encoder (no fitting on test data)\n\nencoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(cat_cols))  \n# Convert encoded test data array into a DataFrame with proper column names\n\ntest_df = pd.concat([test_df.drop(columns=cat_cols), encoded_test_df], axis=1)  \n# Drop original categorical columns from test_df and add the one-hot encoded columns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:38:04.505818Z","iopub.execute_input":"2025-05-20T15:38:04.506414Z","iopub.status.idle":"2025-05-20T15:38:05.428534Z","shell.execute_reply.started":"2025-05-20T15:38:04.506388Z","shell.execute_reply":"2025-05-20T15:38:05.427944Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"X = train_df.drop(columns=[\"id\", \"Calories\"])\ny = train_df[\"Calories\"]\n\nX_test = test_df.drop(columns=[\"id\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:38:10.069165Z","iopub.execute_input":"2025-05-20T15:38:10.069843Z","iopub.status.idle":"2025-05-20T15:38:10.141973Z","shell.execute_reply.started":"2025-05-20T15:38:10.069821Z","shell.execute_reply":"2025-05-20T15:38:10.141419Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Hyperparameters\nxgb_params =  {\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"tree_method\": \"gpu_hist\",\n    'learning_rate': 0.02, \n    'max_depth': 10, \n    'subsample': 0.8, \n    'colsample_bytree': 0.8, \n    \"random_state\": SEED\n}\n\ncat_params = {\n    \"loss_function\": \"RMSE\",\n    \"learning_rate\": 0.03,\n    \"depth\": 10,\n    \"l2_leaf_reg\": 3.0,\n    \"bootstrap_type\": \"Bayesian\",\n    \"bagging_temperature\": 1.0, \n    \"random_seed\": SEED,\n    \"verbose\": 0,\n    \"task_type\": \"GPU\"\n}\n\nlgb_params = {\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.02,\n    \"n_estimators\": 3000, \n    \"num_leaves\": 20,  \n    \"max_depth\": 8, \n    \"min_child_samples\": 20, \n    \"min_split_gain\": 0.01,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.8,\n    \"reg_alpha\": 1.0, \n    \"reg_lambda\": 1.0,\n    \"random_state\": SEED,\n    \"verbosity\": -1,\n    \"feature_fraction\": 0.7,\n    \"force_col_wise\": True\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:38:13.002045Z","iopub.execute_input":"2025-05-20T15:38:13.002701Z","iopub.status.idle":"2025-05-20T15:38:13.007597Z","shell.execute_reply.started":"2025-05-20T15:38:13.002679Z","shell.execute_reply":"2025-05-20T15:38:13.006895Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"The dictionary `xgb_params` sets the hyperparameters for an XGBoost model. The `\"objective\": \"reg:squarederror\"` tells the model it’s solving a regression problem using squared error as the loss function. `\"eval_metric\": \"rmse\"` specifies that the model’s performance will be evaluated using Root Mean Squared Error. `\"tree_method\": \"gpu_hist\"` means the model will use a fast histogram-based algorithm that runs on the GPU for faster training; this method speeds up split finding by grouping continuous features into discrete bins. The `'learning_rate': 0.02` controls how much the model updates at each step, with a smaller value making learning slower but more precise. `'max_depth': 10` limits how deep each decision tree can grow, controlling model complexity. `'subsample': 0.8` defines the fraction (80%) of the training samples randomly selected for building each tree, which helps prevent overfitting by introducing randomness. `'colsample_bytree': 0.8` sets the fraction (80%) of features (columns) randomly chosen for constructing each tree, adding further randomness to reduce overfitting. Finally, `\"random_state\": SEED` ensures the results are reproducible by fixing the random number generator’s seed.\n\n\nThe `cat_params` dictionary contains hyperparameters for training a CatBoost regression model. The `\"loss_function\": \"RMSE\"` indicates that the model is optimized to minimize the Root Mean Squared Error, a common measure of prediction accuracy for regression tasks. `\"learning_rate\": 0.03` controls how much the model updates its predictions in each iteration—lower values mean slower but more stable learning. `\"depth\": 10` specifies the maximum depth of each decision tree, influencing how complex each tree can be. `\"l2_leaf_reg\": 3.0` applies L2 regularization to prevent overfitting by penalizing large weights in the leaves of the trees; L2 regularization works by adding a penalty equal to the square of the magnitude of coefficients, encouraging the model to prefer smaller, more generalizable values. `\"bootstrap_type\": \"Bayesian\"` tells the model to use Bayesian bootstrapping, which adds randomness in a controlled, probabilistic way based on prior uncertainty in the data, helping to reduce overfitting and improve robustness. The `\"bagging_temperature\": 1.0` is a parameter specific to Bayesian bootstrapping; it controls how uniform or aggressive the sampling is—higher values increase randomness in sample weights, allowing the model to focus more on uncertain or mispredicted data points. `\"random_seed\": SEED` ensures that results are reproducible. `\"verbose\": 0` turns off detailed output during training to keep logs clean, and `\"task_type\": \"GPU\"` enables GPU acceleration for faster training, which is especially useful when working with large datasets or deep trees.\n\n\n\nThe `lgb_params` dictionary defines the hyperparameters used to configure a LightGBM regression model. The `\"objective\": \"regression\"` specifies that the task is to predict continuous numerical values. `\"metric\": \"rmse\"` sets the evaluation metric to Root Mean Squared Error, which measures the average difference between predicted and actual values. `\"learning_rate\": 0.02` controls how much the model updates at each step—a smaller value makes learning slower but can improve accuracy. `\"n_estimators\": 3000` means the model can build up to 3000 trees, allowing it to learn complex patterns if needed. `\"num_leaves\": 20` sets the maximum number of leaf nodes per tree; more leaves allow capturing more detail but increase the risk of overfitting. `\"max_depth\": 8` limits how deep each tree can grow to control model complexity. `\"min_child_samples\": 20` ensures that each leaf has at least 20 data points, which helps reduce overfitting. `\"min_split_gain\": 0.01` requires a minimum improvement in the loss function before a split is made, preventing unnecessary splits.\n\nThe `\"subsample\": 0.8` parameter means 80% of the data will be randomly sampled to build each tree, helping prevent overfitting. Similarly, `\"colsample_bytree\": 0.8` means that only 80% of the features will be used for each tree, encouraging model diversity. `\"reg_alpha\": 1.0` and `\"reg_lambda\": 1.0` apply L1 and L2 regularization, respectively. Regularization prevents overfitting by penalizing overly complex models—L1 can zero out some parameters, and L2 shrinks them gradually. `\"random_state\": SEED` ensures consistent results across runs. `\"verbosity\": -1` turns off logs for cleaner output. `\"feature_fraction\": 0.7` limits the fraction of features used at each iteration, similar to `colsample_bytree`.\n\nLastly, `\"force_col_wise\": True` enables **column-wise histogram construction**, which means LightGBM scans and processes one feature (column) at a time to build histograms used in split decisions. This method is more memory-efficient and faster than row-wise methods, especially when dealing with datasets that have many columns.\n\n","metadata":{}},{"cell_type":"code","source":"# Set up repeated K-Fold cross-validation with N_SPLITS splits, repeated N_REPEATS times\nrkf = RepeatedKFold(n_splits=N_SPLITS, n_repeats=N_REPEATS, random_state=SEED)\n\n# Initialize arrays to store out-of-fold (OOF) predictions for each model\n# OOF predictions are made on validation data during cross-validation and help estimate model performance on unseen data\noof_preds_xgb = np.zeros(len(X))\noof_preds_cat = np.zeros(len(X))\noof_preds_lgb = np.zeros(len(X))\n\n# Initialize lists to collect predictions on the test set for each model\ntest_preds_xgb = []\ntest_preds_cat = []\ntest_preds_lgb = []\n\nfor fold, (train_idx, val_idx) in enumerate(rkf.split(X)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    # Transform the target variable using log1p\n    y_train_log = np.log1p(y_train)\n    y_val_log = np.log1p(y_val)\n\n    # XGBoost\n\n    # Convert training and validation data into DMatrix format (optimized format for XGBoost)\n    dtrain = xgb.DMatrix(X_train, label=y_train_log)\n    dval = xgb.DMatrix(X_val, label=y_val_log)\n    \n    # Train the XGBoost model with defined parameters\n    model_xgb = xgb.train(\n    params=xgb_params,               # Hyperparameters for the model\n    dtrain=dtrain,                   # Training data\n    num_boost_round=3000,            # Maximum number of boosting rounds\n    evals=[(dtrain, \"train\"), (dval, \"valid\")],  # Show evaluation metrics for train and validation\n    early_stopping_rounds=100,       # Stop training early if no improvement in 100 rounds\n    verbose_eval=0                   # Don't print output during training\n    )\n\n    # Make predictions on validation set using the best iteration (based on early stopping)\n    xgb_score = model_xgb.predict(dval, iteration_range=(0, model_xgb.best_iteration))\n        \n    # Save predictions in the correct places (out-of-fold predictions)\n    oof_preds_xgb[val_idx] = xgb_score\n        \n    # If log1p was used, we might need to reverse it with expm1 to get actual values\n    # oof_preds_xgb[val_idx] = np.expm1(xgb_score)\n        \n    # Prepare test data for prediction\n    dtest = xgb.DMatrix(X_test)\n\n    # Predict on test set using the trained model\n    xgb_pred = model_xgb.predict(dtest, iteration_range=(0, model_xgb.best_iteration))\n    \n    # Save test predictions for this fold\n    test_preds_xgb.append(xgb_pred)\n    \n    # Again, if log1p was used earlier, we may reverse it here\n    # test_preds_xgb.append(np.expm1(xgb_pred))\n\n    # CatBoost\n    model_cat = CatBoostRegressor(**cat_params)\n    model_cat.fit(X_train, y_train_log, verbose=False)\n    cat_score = model_cat.predict(X_val)\n    oof_preds_cat[val_idx] = cat_score\n    # oof_preds_cat[val_idx] = np.expm1(cat_score)\n\n    cat_pred = model_cat.predict(X_test)\n    test_preds_cat.append(cat_pred)\n    # test_preds_cat.append(np.expm1(cat_pred))\n\n    \n    # LGBM\n    model_lgb = LGBMRegressor(**lgb_params)\n    model_lgb.fit(X_train, y_train_log)\n    lgb_score = model_lgb.predict(X_val)\n    oof_preds_lgb[val_idx] = lgb_score\n    # oof_preds_lgb[val_idx] = np.expm1(lgb_score)\n\n    lgb_pred = model_cat.predict(X_test)\n    test_preds_lgb.append(lgb_pred)\n    # test_preds_lgb.append(np.expm1(lgb_pred))\n\n    # Calculate the score by comparing with y_val before the log1p transformation\n    avg_score = (np.expm1(xgb_score) + np.expm1(cat_score) + np.expm1(lgb_score)) / 3\n    print(f'===============Fold: {fold+1} Average RMSLE score: {np.mean(rmsle_score(y_val, avg_score)):.5f}') \n    print(f'-------------------->  XGBoost RMSLE score: {np.mean(rmsle_score(y_val, np.expm1(xgb_score))):.5f}') \n    print(f'--------------------> CatBoost RMSLE score: {np.mean(rmsle_score(y_val, np.expm1(cat_score))):.5f}') \n    print(f'--------------------> LightGBM RMSLE score: {np.mean(rmsle_score(y_val, np.expm1(lgb_score))):.5f}') \n    print('')\n\n    # Creating train and val data for stacking\n    stacked_train = np.vstack([oof_preds_xgb, oof_preds_cat, oof_preds_lgb]).T\n    stacked_test = np.vstack([np.mean(test_preds_xgb, axis=0),\n                              np.mean(test_preds_cat, axis=0),\n                              np.mean(test_preds_lgb, axis=0)\n                             ]).T\n       \n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:38:20.711962Z","iopub.execute_input":"2025-05-20T15:38:20.712416Z","iopub.status.idle":"2025-05-20T15:55:03.608958Z","shell.execute_reply.started":"2025-05-20T15:38:20.712394Z","shell.execute_reply":"2025-05-20T15:55:03.608255Z"}},"outputs":[{"name":"stdout","text":"===============Fold: 1 Average RMSLE score: 0.05924\n-------------------->  XGBoost RMSLE score: 0.05992\n--------------------> CatBoost RMSLE score: 0.05928\n--------------------> LightGBM RMSLE score: 0.06003\n\n===============Fold: 2 Average RMSLE score: 0.05973\n-------------------->  XGBoost RMSLE score: 0.06009\n--------------------> CatBoost RMSLE score: 0.05989\n--------------------> LightGBM RMSLE score: 0.06063\n\n===============Fold: 3 Average RMSLE score: 0.05902\n-------------------->  XGBoost RMSLE score: 0.05946\n--------------------> CatBoost RMSLE score: 0.05933\n--------------------> LightGBM RMSLE score: 0.05992\n\n===============Fold: 4 Average RMSLE score: 0.05957\n-------------------->  XGBoost RMSLE score: 0.05972\n--------------------> CatBoost RMSLE score: 0.05975\n--------------------> LightGBM RMSLE score: 0.06062\n\n===============Fold: 5 Average RMSLE score: 0.05893\n-------------------->  XGBoost RMSLE score: 0.05932\n--------------------> CatBoost RMSLE score: 0.05917\n--------------------> LightGBM RMSLE score: 0.05972\n\n===============Fold: 6 Average RMSLE score: 0.05916\n-------------------->  XGBoost RMSLE score: 0.05949\n--------------------> CatBoost RMSLE score: 0.05942\n--------------------> LightGBM RMSLE score: 0.06008\n\n===============Fold: 7 Average RMSLE score: 0.05877\n-------------------->  XGBoost RMSLE score: 0.05908\n--------------------> CatBoost RMSLE score: 0.05895\n--------------------> LightGBM RMSLE score: 0.05977\n\n===============Fold: 8 Average RMSLE score: 0.06100\n-------------------->  XGBoost RMSLE score: 0.06186\n--------------------> CatBoost RMSLE score: 0.06108\n--------------------> LightGBM RMSLE score: 0.06169\n\n===============Fold: 9 Average RMSLE score: 0.05829\n-------------------->  XGBoost RMSLE score: 0.05863\n--------------------> CatBoost RMSLE score: 0.05851\n--------------------> LightGBM RMSLE score: 0.05926\n\n===============Fold: 10 Average RMSLE score: 0.05897\n-------------------->  XGBoost RMSLE score: 0.05927\n--------------------> CatBoost RMSLE score: 0.05926\n--------------------> LightGBM RMSLE score: 0.05982\n\n===============Fold: 11 Average RMSLE score: 0.05920\n-------------------->  XGBoost RMSLE score: 0.05963\n--------------------> CatBoost RMSLE score: 0.05935\n--------------------> LightGBM RMSLE score: 0.06029\n\n===============Fold: 12 Average RMSLE score: 0.05859\n-------------------->  XGBoost RMSLE score: 0.05892\n--------------------> CatBoost RMSLE score: 0.05873\n--------------------> LightGBM RMSLE score: 0.05949\n\n===============Fold: 13 Average RMSLE score: 0.05942\n-------------------->  XGBoost RMSLE score: 0.05962\n--------------------> CatBoost RMSLE score: 0.05966\n--------------------> LightGBM RMSLE score: 0.06062\n\n===============Fold: 14 Average RMSLE score: 0.05969\n-------------------->  XGBoost RMSLE score: 0.06077\n--------------------> CatBoost RMSLE score: 0.05951\n--------------------> LightGBM RMSLE score: 0.06013\n\n===============Fold: 15 Average RMSLE score: 0.05944\n-------------------->  XGBoost RMSLE score: 0.05972\n--------------------> CatBoost RMSLE score: 0.05971\n--------------------> LightGBM RMSLE score: 0.06042\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"This above code combines the predictions from three different models—XGBoost, CatBoost, and LightGBM—by stacking their out-of-fold predictions vertically and then transposing the result to create a new training dataset (`stacked_train`). It does the same with the averaged test predictions to form `stacked_test`. This process, called **stacking, involves using the predictions of multiple models as input features for a new model, which can improve overall accuracy by learning from the strengths of each individual model.**\n","metadata":{}},{"cell_type":"code","source":"# Prediction - Using multiple meta models\n\ny_log = np.log1p(y)\n\n# Ridge\nridge = RidgeCV(alphas=[0.1, 1.0, 10.0, 50.0, 100.0], cv=5)\nridge.fit(stacked_train, y_log)\nridge_preds = ridge.predict(stacked_test)\n\n#LinearRegression\nlr = LinearRegression()\nlr.fit(stacked_train, y_log)\nlr_preds = lr.predict(stacked_test)\n\n# BayesianRidge\nbayesian_ridge = BayesianRidge(n_iter=500, tol=1e-3, alpha_1=1e-6, alpha_2=1e-6, lambda_1=1e-6, lambda_2=1e-6)\nbayesian_ridge.fit(stacked_train, y_log)\nbayes_preds = bayesian_ridge.predict(stacked_test)\n\n# Averaging\nfinal_preds = (ridge_preds + lr_preds + bayes_preds) / 3\nfinal_preds = np.expm1(final_preds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:02.196710Z","iopub.execute_input":"2025-05-20T15:57:02.197248Z","iopub.status.idle":"2025-05-20T15:57:03.647699Z","shell.execute_reply.started":"2025-05-20T15:57:02.197206Z","shell.execute_reply":"2025-05-20T15:57:03.647089Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"This code segment above details the final prediction phase of a machine learning workflow, employing a meta-modeling or stacking approach where predictions from multiple models are combined. Initially, the target variable y is transformed using np.log1p(y) (which computes log(1+x)), likely to handle skewed data or improve model performance by making the distribution more normal; this means all subsequent model training occurs on this log-transformed target. Three different meta-models are then trained on stacked_train, which represents the out-of-fold predictions from a previous layer of base models. First, a RidgeCV model is fitted, automatically selecting the optimal alpha (regularization strength) from a predefined list ([0.1, 1.0, 10.0, 50.0, 100.0]) through 5-fold cross-validation. Second, a standard LinearRegression model is trained, providing a straightforward linear combination of the stacked features. Third, a BayesianRidge model is fit with specific regularization parameters (n_iter, tol, alpha_1, alpha_2, lambda_1, lambda_2), which uses a probabilistic approach to estimate the coefficients and the regularization terms. After training, each of these three meta-models generates predictions on the stacked_test dataset. Finally, the **final_preds are calculated by averaging the predictions from the RidgeCV, LinearRegression, and BayesianRidge models**. This ensemble technique, simple averaging, aims to leverage the strengths of each meta-model and reduce the variance of the overall prediction. The very last step, np.expm1(final_preds), reverses the initial log transformation (e \nx\n −1) to convert the predictions back to their original scale, making them interpretable and directly comparable to the untransformed target variable.","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id': test_df['id'],\n    'Calories': final_preds\n})\n\n# Save\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:22.896980Z","iopub.execute_input":"2025-05-20T15:57:22.897738Z","iopub.status.idle":"2025-05-20T15:57:23.374816Z","shell.execute_reply.started":"2025-05-20T15:57:22.897713Z","shell.execute_reply":"2025-05-20T15:57:23.374281Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:57:32.314352Z","iopub.execute_input":"2025-05-20T15:57:32.314869Z","iopub.status.idle":"2025-05-20T15:57:32.331936Z","shell.execute_reply.started":"2025-05-20T15:57:32.314848Z","shell.execute_reply":"2025-05-20T15:57:32.331346Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"            id    Calories\n0       750000   27.194003\n1       750001  107.511334\n2       750002   87.539132\n3       750003  125.150482\n4       750004   76.507740\n...        ...         ...\n249995  999995   26.158479\n249996  999996    9.531712\n249997  999997   73.634638\n249998  999998  167.652428\n249999  999999   76.574843\n\n[250000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Calories</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>750000</td>\n      <td>27.194003</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>750001</td>\n      <td>107.511334</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>750002</td>\n      <td>87.539132</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>750003</td>\n      <td>125.150482</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>750004</td>\n      <td>76.507740</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>249995</th>\n      <td>999995</td>\n      <td>26.158479</td>\n    </tr>\n    <tr>\n      <th>249996</th>\n      <td>999996</td>\n      <td>9.531712</td>\n    </tr>\n    <tr>\n      <th>249997</th>\n      <td>999997</td>\n      <td>73.634638</td>\n    </tr>\n    <tr>\n      <th>249998</th>\n      <td>999998</td>\n      <td>167.652428</td>\n    </tr>\n    <tr>\n      <th>249999</th>\n      <td>999999</td>\n      <td>76.574843</td>\n    </tr>\n  </tbody>\n</table>\n<p>250000 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"The overall effect of this intricate machine learning workflow is to significantly enhance prediction accuracy and robustness for a regression task by leveraging the strengths of multiple diverse models through a sophisticated **stacking ensemble method**.\n\nThe process begins with **data preparation**, specifically applying a `np.log1p` transformation to the target variable `y`. This is **crucial for handling potentially skewed target distributions**, common in real-world regression problems. By transforming `y` to $\\log(1+y)$, the distribution often becomes more symmetrical and normal-like, making it easier for models to learn patterns and **reduce the impact of outliers**.\n\nNext, the workflow implements a **two-layer stacking architecture**. The **first layer, consisting of \"base models\" (XGBoost, CatBoost, and LightGBM)**, generates **\"out-of-fold predictions.\"** This is a **critical technique to prevent data leakage**: each base model predicts on data it *did not* see during its own training. This ensures the \"meta-features\" derived from these predictions are **unbiased and truly represent generalization ability**. These out-of-fold predictions for the training set are then combined (stacked) to form `stacked_train`, and similarly, averaged test predictions from the base models form `stacked_test`.\n\nThe **second layer, the \"meta-modeling\" phase**, then trains simpler models (**`RidgeCV`**, **`LinearRegression`**, and **`BayesianRidge`**) on these `stacked_train` features. These meta-models learn how to **optimally combine the predictions of the base models**, recognizing their individual strengths and weaknesses. For instance, `RidgeCV` automatically tunes its regularization, `LinearRegression` provides a straightforward blend, and `BayesianRidge` offers a probabilistic, more robust combination.\n\nFinally, the predictions from these three meta-models are **simply averaged** to produce `final_preds`. This final averaging step acts as an **additional ensemble layer**, further smoothing out individual model biases and **reducing variance**, leading to a more stable and often more accurate final prediction. The very last step, `np.expm1`, is **essential to revert the initial log transformation** ($e^x - 1$), returning the predictions to their original scale, making them directly comparable to the true values and easily interpretable.\n\n---\n\nIn summary, this process creates a **powerful predictive system** by:\n* **Preprocessing the target variable** for better model learning.\n* **Generating diverse \"meta-features\"** from robust, out-of-fold predictions of strong base models (XGBoost, CatBoost, LightGBM).\n* **Learning intelligent combinations** of these meta-features using different linear meta-models.\n* **Ensembling the meta-model predictions** for increased stability and accuracy.\n* **Transforming back** the final predictions to their original, interpretable scale.\n\nThis multi-stage approach, particularly the use of **stacking with out-of-fold predictions**, is a highly effective strategy in competitive machine learning for achieving state-of-the-art performance by capitalizing on the **collective intelligence of multiple diverse algorithms**.","metadata":{}}]}